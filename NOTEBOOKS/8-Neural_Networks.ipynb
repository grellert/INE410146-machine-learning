{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e30bb8b",
   "metadata": {},
   "source": [
    "### INE 410146 - Applied Machine Learning\n",
    "$\\textbf{Author: Prof. Mateus Grellert}$\n",
    "\n",
    "Bibliography used in this lesson:\n",
    "- Aggarwal, Charu C. Data mining: the textbook. Springer, 2015 - Chapter 10.7\n",
    "- Aggarwal, Charu C. Neural networks and deep learning, 2018 - Chapter 1\n",
    "- Luger, George F. Artificial intelligence: structures and strategies for complex problem solving. Pearson education, 2005 - Chapter 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9da523",
   "metadata": {},
   "source": [
    "# 8 - Neural Networks\n",
    "\n",
    "Neural Networks (NN) are special types of models that try to mimic the way our brain operates. Our brain is very efficient computing mechanism that operates using a network of several nerve cells called neurons. Each neuron has a very simple mechanism that receives electrical/chemical stimuli and eventually fires signals to other neurons through connections called synapses. These connections can either be excitatory (meaning synapses promote the generation of an electrical signal) or inhibitory (synapses prevent the generation of electrical signals). Our learning process consists in changing the strength in these synactic connections. \n",
    "\n",
    "\n",
    "NN follow the same idea of creating a connected network of simple units that resemble our biological neurons. For this reason, the terms ***connectionist*** and ***bio-inspired*** learning are commonly used when referring to NNs. It is also because of this analogy that some authors use the term **Artificial Neural Networks (ANN)** to represent the mathematical model. The figure below shows a comparison between a Biological and an Artificial Neural Network.\n",
    "\n",
    "<center>\n",
    "<img src =\"FIGS/8-biological_artificial_neuron.png\" width=\"600px\"><br>(source: Aggarwal, Charu C. \"Neural networks and deep learning.\" (2018))\n",
    "</center>\n",
    "    \n",
    "On the right side of this figure, we have the input channels of the neurons (called dendrites), receiving stimuli from other units. These stimuli are weighted ($w_1$, $w_2$, ...) before being passed to the core of the neuron, which represents the strength in synaptic connexions. The inputs can come from other neurons (thus the term network), or from external data (which would be analogous to our biological sensors).\n",
    "\n",
    "In the next sessions, we will discuss how these models can be automatically trained to learn the correct weights of each input.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "The Perceptron was one of the first mathematical model of a neuron and is the basis of modern NNs to this day. This model was proposed in 1958 by a psychologist named Frank Rosenblatt, while he was working as a researcher at the Cornell Aeronautical Laboratory. The figure below shows an illustration of the Perceptron.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"500px\">\n",
    "<br> (source: Medium)\n",
    "</center>\n",
    "\n",
    "The Perceptron consists of four parts (mapped in the figure):\n",
    "- A set of input values that are represented as an input layer (blue circles)\n",
    "- Weights associated to each input value (green circles)  and a bias term (constant*$w_0$)\n",
    "- An accumulating unit that adds up the weighted inputs (first red circle)\n",
    "- An activation (or threshold) function that translates the accumulated value to an output (second red circle). The threshold function is intended to produce the on/off state of actual neurons.\n",
    "\n",
    "The weights are multiplied with each input before being fed to the accumulator. This also means that the first component of a Perceptron is a **Multiply-and-Accumulate (MAC)** operation.\n",
    "\n",
    "The activation function of a Perceptron varies depending on the reference, but it is either a unitary step (or heaviside) function or a sign function, both of which map an input value to two possible outputs. We will use the former in this lesson, but the equations for both can be seen below, where $H$ is the heavised function.\n",
    "\n",
    "<div style=\"display: flex;margin:10px\">\n",
    "<div style=\"margin-right: 50px\">\n",
    "$sign(x) =   \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      -1, ~\\mathrm{if } x \\leq 0\\\\\n",
    "      +1 ~\\mathrm{ otherwise}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$\n",
    "</div>\n",
    "<div>\n",
    "$H(x) =   \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      0, ~\\mathrm{if } x \\leq 0\\\\\n",
    "      1 ~\\mathrm{ otherwise}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "As we can observe, the Perceptron was originally designed to model **binary classification** problems, as it mimics the on-off effect of neural synapses. However, Perceptrons can also be used for regression by removing the activation function, using the MAC result as output. \n",
    "\n",
    "Putting it all together, considering a data set $D$ containing $N$ input samples with that we will call $X$, each containing $k$ features, and a label vector $y$, we can define the mathematical model of the perceptron as folows:\n",
    "\n",
    "$ z_i = H\\big(\\sum\\limits_{j=1}^k({w_j*x_{i,j})+b}\\big)$, or similarly in matrix notation, <br>\n",
    "$ z_i = H(\\bar{W}\\bar{X_i}+b)$\n",
    "\n",
    "Where $z_i$ is the output for the $X_i$ input sample. Note that the first term represents the MAC operation, which as added with the bias value. As the second. equation shows, we can also represent this MAC operation using the **dot product** of the $\\bar{W}$ and $\\bar{X_i}$ arrays. A final observation is that the Perceptron, like the Linear/Logistic regressions, is a **linear model**, meaning it will define a line, a plane or a hyperplane to separate our data set. \n",
    "\n",
    "Now let's remember the fact that we want to learn a given task with this model, so we need to somehow adjust it to fit our input samples into expected output values. The quality of this fit is measured with a **loss function** that computes how wrong our predictions are. Therefore, we must minimize this loss function to obtain a good model. The loss function used in the Perceptron algorithm is called **0-1 loss**, and is defined as follows:\n",
    "\n",
    "$L_{0-1}(y,\\hat{y}) = \\sum\\limits_{i=1}^N|y_i-\\hat{y}_i|$\n",
    "\n",
    "We can interpret the $L_{0-1}$ loss as simply being the total amount of wrong predictions.\n",
    "\n",
    "Now that we have a model for our neuron and a loss function to evaluate it, we can adjust the $\\bar{W}$ and $b$ terms of our model to minimize this loss. The question is: ***how can we do that without testing every possible combination?***\n",
    "\n",
    "The Perceptron **learning algorithm**, has a clever weight update rule that updates the weights based on an implicit smooth approximation of the gradient of the **$L_2$-loss function**. In other words, it employs a type of gradient descent. Let's check the formula:\n",
    "\n",
    "$W' = W+\\alpha(y-\\hat{y})*X$\n",
    "\n",
    "When the values between $y$ and $\\hat{y}$ are the same (correct predictions), the difference becomes $0$ and no update is applied. When they are different, the weights go up or down to follow the gradient of the $L_2$-loss. \n",
    "\n",
    "This update occurs for each input sample in our data set, and each complete pass is called an **epoch**. The training is repeated until convergence (loss becomes zero), or when the maximum number of epochs is achieved.\n",
    "\n",
    "The $\\alpha$ term is called the **learning rate**. It controls how agressively the weights will be updated when mispredictions occur. Just like with gradient descent, high values of $\\alpha$ will lead to faster convergence, but can also cause divergence. Small values of $\\alpha$ take longer to converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2d4b3",
   "metadata": {},
   "source": [
    "Now we will see all of these concepts in code. First, let's set up an example data set with the help of the ``make_classification()`` function. An important note here is that Perceptrons and NNs in general require data to be **normalized** so that each input feature falls within the same range. Since this is already guaranteed with the ``make_classification()`` function, we can simply disregard this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74951bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100, class_sep = 1.5,\n",
    "                                    n_features=2, n_informative=2, \n",
    "                                    n_redundant=0, n_classes=2)\n",
    "\n",
    "plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'g^')\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68b440",
   "metadata": {},
   "source": [
    "Now we will implement the followinc necessary functions:\n",
    " - $L_{0-1}$ loss: implements the 0-1 loss function\n",
    " - Step function (heaviside): activation function of our neuron\n",
    " - **prediction**: receives the input samples and the network parameters (weights) to compute the predicted output.\n",
    " - **perceptron train**: implements the training algorithm using the weight update rule previously discussed\n",
    " - **plot decision boundary**: a helper function that implements the visualization of the decision function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac189951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from: \n",
    "# https://towardsdatascience.com/perceptron-algorithm-in-python-f3ac89d2e537\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# computes the 0-1 loss for given an array of real (y) and predicted (y_hat) values\n",
    "def zero_one_loss(y, y_hat):\n",
    "    return (y.astype(int) != y_hat.astype(int)).sum()\n",
    "\n",
    "# heaviside step function implementation (using matrix operations)\n",
    "def step_func(z):\n",
    "        return (z > 0).astype(int)\n",
    "    \n",
    "# predicting consists of applying the MAC + activation function over the input samples\n",
    "def predict(X, weights):\n",
    "    # creating one column for the bias term\n",
    "    bias = np.ones(X.shape[0])\n",
    "    X_bias = np.hstack((bias.reshape(-1,1), X))\n",
    "    y_hat = []\n",
    "    for x_i,y_i in zip(X_bias,y):            \n",
    "        # Calculating prediction/hypothesis.\n",
    "        y_hat_i = step_func(np.dot(x_i.T, weights))\n",
    "        y_hat.append(y_hat_i)\n",
    "\n",
    "    return np.array(y_hat).ravel()\n",
    "\n",
    "def perceptron_train(X, y, lr, epochs):\n",
    "    \n",
    "    # X --> Inputs.\n",
    "    # y --> labels/target.\n",
    "    # lr --> learning rate.\n",
    "    # epochs --> Number of iterations.\n",
    "    \n",
    "    # m-> number of training examples\n",
    "    # n-> number of features \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing parapeters(weights) to zeros.\n",
    "    # +1 in n+1 for the bias term.\n",
    "    weights = np.zeros((n+1,1))\n",
    "    \n",
    "    # Empty list to store how many examples were \n",
    "    # misclassified at every iteration.\n",
    "    n_miss_list = []\n",
    "    \n",
    "    # Training.\n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        # variable to store #misclassified.\n",
    "        n_miss = 0\n",
    "        \n",
    "        # looping for every example.\n",
    "        for idx, x_i in enumerate(X):\n",
    "            \n",
    "            # Insering 1 for bias, X0 = 1.\n",
    "            x_i = np.insert(x_i, 0, 1).reshape(-1,1)\n",
    "            \n",
    "            # Calculating prediction/hypothesis.\n",
    "            y_hat = step_func(np.dot(x_i.T, weights))\n",
    "            \n",
    "            # Updating if the example is misclassified.\n",
    "            if (np.squeeze(y_hat) - y[idx]) != 0:\n",
    "                weights += lr*((y[idx] - y_hat)*x_i)\n",
    "                \n",
    "                # Incrementing by 1.\n",
    "                n_miss += 1\n",
    "        \n",
    "        # Appending number of misclassified examples\n",
    "        # at every iteration.\n",
    "        if epoch % 10 == 0:\n",
    "            y_hat = predict(X, weights)\n",
    "            loss = zero_one_loss(y, y_hat)\n",
    "            print(f'Epoch {epoch}, 0-1 loss {loss}')\n",
    "            \n",
    "        # early termination when loss reaches 0\n",
    "        if n_miss == 0:\n",
    "            break\n",
    "\n",
    "    return weights\n",
    "\n",
    "def plot_decision_boundary(X, weights):\n",
    "    \n",
    "    # X --> Inputs\n",
    "    # weights --> parameters\n",
    "    \n",
    "    # The Line is y=mx+c\n",
    "    # So, Equate mx+c = weights0.X0 + weights1.X1 + weights2.X2\n",
    "    # Solving we find m and c\n",
    "    x1 = [min(X[:,0]), max(X[:,0])]\n",
    "    m = -weights[1]/weights[2]\n",
    "    c = -weights[0]/weights[2]\n",
    "    x2 = m*x1 + c\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"r^\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
    "    plt.xlabel(\"feature 1\")\n",
    "    plt.ylabel(\"feature 2\")\n",
    "    plt.title(\"Perceptron Algorithm\")\n",
    "    plt.ylim((X[:, 1].min()-1, X[:, 1].max()+1))\n",
    "    plt.tight_layout()\n",
    "    plt.plot(x1, x2, 'y-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808c46b",
   "metadata": {},
   "source": [
    "Now we are ready to run our Perceptron using the code above. Feel free to try different values of $\\alpha$ and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46da212",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1\n",
    "epochs = 50 \n",
    "\n",
    "weights = perceptron_train(X, y, lr, epochs)\n",
    "y_hat = predict(X, weights)\n",
    "loss = zero_one_loss(y, y_hat)\n",
    "\n",
    "print(f'Final loss: {loss}')\n",
    "plot_decision_boundary(X, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f0acb",
   "metadata": {},
   "source": [
    "If our data is **linearly separable**, we can fit a perfect line that separates both groups using the Perceptron algorithm. However, in most cases our data is not linearly separable, so a linear model like the Perceptron will not be able to fit a model that completely separates the two groups. That is why we introduce the number of epochs as a stopping criterion as well, since convergence cannot be completely obtained.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f36bd8e2",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons (MLP)\n",
    "\n",
    "What about nonlinear models? As we mentioned, our data is usually not linearly separable, so linear models like the Perceptron are not very efficient.\n",
    "\n",
    "To solve this, the Multilayer Perceptrons (MLP) were created. As the name suggests, we use several Perceptrons instead of one to creat MLPs. Therefore, we have at least three layers in a MLP: (i) an input layer; (ii) a hidden layer containing 2 or more perceptrons; and (iii) an output layer. Is it also possible to use more than one hidden layer. In this casse the outputs from the previous layer are used as inputs to the next one, configuring a **feed-forward network**. The figure below shows an example of a MLP with a single hidden layer.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Mohamed-Zahran-16/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png\" width=\"500px\">\n",
    "<br> (source: Mohamed, H., et al. \"Assessment of artificial neural network for bathymetry estimation using High Resolution Satellite imagery in Shallow Lakes: case study El Burullus Lake.\" International water technology conference. 2015)\n",
    "</center>\n",
    "\n",
    "The main idea is that the different nodes in the hidden layer can capture different decision boundaries in different regions of the data, and the node in the output layer can combine the results from these different decision boundaries.\n",
    " \n",
    "A second important observation that differs MLP from single Perceptrons is that the **activation functions used in each node of the MLP cannot be linear** (like the threshold/step function). This is a necessary constraint that allows us to model nonlinear problems with this network.\n",
    "\n",
    "Traditionally, two types of activations are used, both of which are sigmoid functions: the hiperbolic tangent ($\\mathrm{tanh}$) and the logistic function. They are both sigmoid functions, meaning they map the input to an S-shaped curve, but $\\mathrm{tanh}$ maps the values to $-1$ and $1$, whereas the logistic maps values to a $0,1$ interval. The figures below show the equation and the plot of each function.\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "    <th>  logistic </th>   <th>  tanh </th> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> $L(x) = \\frac{1}{1+e^{-cx}}$  </td> <td>  $tanh(x) = \\frac{senh(x)}{cosh(x)} = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$  </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"FIGS/8-logistic.png\" width=300px /> </td> <td> <img src=\"FIGS/8-tanh.png\" width=300px /> \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "While both can be used to solve the same problems and are likely to produce similar results with enough training time and effort, some practioners recommend using tanh over sigmoid (like [here](https://medium.com/analytics-vidhya/activation-functions-why-tanh-outperforms-logistic-sigmoid-3f26469ac0d1)) due to the fact that tanh treats differently strongly negative inputs (mapping them near $-1$) and irrelevant ones (mapping to $0$). \n",
    "\n",
    "Aggarwal mentions two important challenges that arise from using MLPs:\n",
    "\n",
    "- The initial design of the topology of the network presents many **trade-off challenges** for the analyst. A larger number of nodes and hidden layers provides greater generality, but a corresponding risk of overfitting. Little guidance is available about the design of the topology of the neural network because of poor interpretability associated with the multilayer neural network classification process. While some hill climbing methods can be used to provide a limited level of learning of the correct neural network topology,the issue of good neural network design still remains somewhat of an open question.\n",
    "- Neural networks are **slow to train** and sometimes sensitive to noise. Thousands of epochs may be required to train a multilayer neural network. A larger network is likely to have a very slow learning process. While the training process of a neural network is slow, it is relatively efficient to classify test instances.\n",
    "\n",
    "Another problem that arises with the use of MLPs is the fact that we cannot employ the Perceptron update rule, because it doesn't give us a way of updating the weights of the intermediate hidden layers. That was solved with the backpropagation algorithm shown in the next section.\n",
    "\n",
    "\n",
    "## Backpropagation Algorithm\n",
    "\n",
    "When we were training the Perceptron, it was easy to understand how to update the weights because we had a direct mapping between them and the output. In the case of multilayer networks, the problem is that the ground-truth output of the hidden layer nodes are not known because there are no training labels associated with the outputs of these nodes. \n",
    "\n",
    "Therefore, we need to find a way to make errors on the output layer affect the weights of all the internal nodes of our network. This is achieved with the use of the **backpropagation algorithm**. This algorithm contains two main phases, which are applied in the weight update process for each training instance:\n",
    "\n",
    "- **Forward pass:** the inputs are fed into the network and the output of each node is propagated all the way to the output nodes. In this pass, there is no weight update. We are simply concerned with obtaining the output of the network to check whether or not the predicted label is an error.\n",
    "\n",
    "- **Backward pass:** here the goal is to update the weights in the backward direction by providing an error estimate of the output of a node in the earlier layers, using the errors in later layers. The error estimate of a node in the hidden layer is computed as a function of the error estimates and weights of the nodes in the layer ahead of it. This is then used to compute an **error gradient** with respect to the weights in the node and to update the weights of this node. The **learning rate** is also applied in this step to adjust the strength of each update.\n",
    "\n",
    "The actual update equation is not very different from the basic perceptron at a conceptual level. The only differences that arise are due to the nonlinear functions commonly used in hidden layer nodes, and the fact that errors at hidden-layer nodes are estimated via backpropagation, rather than directly computed by comparison of the output to atraining label. This entire process is propagated backwards to update the weights of all the nodes in the network. The animation below shows an overview of how backpropagation occurs.\n",
    "\n",
    "<center> <br>\n",
    "<img src=\"https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif\" width=400px /> <br> (source: machinelearningknowledge.ai)\n",
    "</center>\n",
    "\n",
    "The catch to propagate the error gradient along the hidden layers of the network is based in the **chain rule** of calculus. The chain rule allows us to find the derivative of composite functions, and the clever trick of the backpropagation algorithm was to model the hidden layers as such composite functions.\n",
    "\n",
    "## Batch, Stochastic and Mini-batch Gradient Descent\n",
    "\n",
    "As we've seen, the backpropagation algorithm gives us a mathematical way of updating the weights of our Neural Network using Gradient Descent and the chain rule as foundations. However, this process can still be quite computing and data intensive, as it involves a googolplex of multiplications and additions in its forward and backward passes. \n",
    "\n",
    "If we update the weights with each input sample, we allow our model to adapt to every type of input we have. At the same time, this requires a significant amount of backward passes, specially if we have huge data sets.\n",
    "\n",
    "There are several strategies that allows us to reduce explore the trade-off between training efficiency and complexity. Let's see how they work:\n",
    "\n",
    "- **Batch Gradient Descent**: in this strategy, all the samples are used in the forward pass, and the backward pass is only computed after this, using the average of the gradients to update the weights. Therefore, we have only one weight update in an epoch. The advantage is that we have a significant reduction in computing time, but we are losing a lot of information with the error aggregation.\n",
    "- **Stochastic Gradient Descent (SGD)**: a second approach consists of computing the forward and backward passes for each input sample. Therefore, our model will update faster and also converge faster.\n",
    "- **Mini-batch Stochastic Gradient Descent**: this is a mid-term between the two. The idea is that we aggrate the gradient over a small subset of input samples (called batches). After each batch, we update our model weights with a backward pass.\n",
    "\n",
    "The table below compares each strategy in terms of number of forware and backward passes computed each epoch for a data set with $N$ samples. The term $B$ represents the batch size.\n",
    "\n",
    "|                |BGD    |SGD    |MB-SGD   |\n",
    "|---|---|---|---|\n",
    "|forward passes  | $N$   | $N$   | $N$     |\n",
    "|backward passes | $1$   | $N$   | $N/B$   |\n",
    "\n",
    "## MLP in Code\n",
    "\n",
    "The code below implements:\n",
    " - a classification MLP\n",
    " - trained to optimize the L2-norm using SGD\n",
    " - uses logistic as activation function in all its nodes\n",
    " \n",
    "This code is an adaptation of a very good [post]( https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/) from the Machine Learning Mastery blog. The only difference is that this version supports multiple layers with arbitrary sizes each.\n",
    "\n",
    "Feel free to study the code and tinker with the MLP parameters. It would also be interesting to try it with a real data set (don't forget to normalize your data though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddecaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Accuracy 0.7655\n",
      "Epoch 20 - Accuracy 0.7745\n",
      "Epoch 30 - Accuracy 0.822\n",
      "Epoch 40 - Accuracy 0.8215\n",
      "Epoch 50 - Accuracy 0.8205\n",
      "Epoch 60 - Accuracy 0.822\n",
      "Epoch 70 - Accuracy 0.821\n",
      "Epoch 80 - Accuracy 0.814\n",
      "Epoch 90 - Accuracy 0.83\n",
      "Epoch 100 - Accuracy 0.8305\n"
     ]
    }
   ],
   "source": [
    "# code adapted from: \n",
    "# https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "# Backprop on the Seeds Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "\n",
    "# Calculate neuron MAC for an input\n",
    "def mult_accum(weights, inputs):\n",
    "    mac = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        mac += weights[i] * inputs[i]\n",
    "    return mac\n",
    "\n",
    "# sigmoid (logistic) activation\n",
    "def logistic(mac):\n",
    "    return 1.0 / (1.0 + exp(-mac))\n",
    "\n",
    "# Calculate the derivative of the logistic output (for gradient computation)\n",
    "def logistic_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# Computes the accuracy of a data set given a trained network\n",
    "def compute_acc(X, y, network):\n",
    "    acc = 0.0\n",
    "    for xi, yi in zip(X,y):\n",
    "        prediction = predict(network, xi)\n",
    "        acc += int(yi == prediction)\n",
    "    return acc/X.shape[0]\n",
    "\n",
    "# Forward propagate a single input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            mac = mult_accum(neuron['weights'], inputs)\n",
    "            neuron['output'] = logistic(mac)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    # for each layer\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            # hidden layers - error = W*delta(z)\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            # last layer - gradient = output - expected (for L2-loss)\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        \n",
    "        # computes the gradient (delta) of the error\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * logistic_derivative(neuron['output'])\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, X, y, l_rate, n_epoch, n_classes):\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for row, label in zip(X, y):\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_classes)]\n",
    "            expected[int(label)] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "            \n",
    "        if (epoch % 10) == 0:\n",
    "            acc = compute_acc(X, y, network)\n",
    "            print(f'Epoch {epoch} - Accuracy {acc}')\n",
    "            \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden_list, n_classes):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden_list[0])]\n",
    "    network.append(hidden_layer)\n",
    "    for i in range(1, len(n_hidden_list)):\n",
    "        hidden_layer = [{'weights':[random() for j in range(n_hidden_list[i-1] + 1)]} for j in range(n_hidden_list[i])]\n",
    "        network.append(hidden_layer)\n",
    "\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden_list[-1] + 1)]} for i in range(n_classes)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(X , y, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = X.shape[1]\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    network = initialize_network(n_inputs, n_hidden, n_classes)\n",
    "    train_network(network, X, y, l_rate, n_epoch, n_classes)\n",
    "    return network\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=2000, class_sep = 0.5,\n",
    "                                    n_features=8, n_informative=4, \n",
    "                                    n_redundant=0, n_classes=2)\n",
    "\n",
    "# evaluate algorithm\n",
    "l_rate = 1\n",
    "n_epoch = 100\n",
    "n_hidden = [10,5]\n",
    "network = back_propagation(X , y, l_rate, n_epoch, n_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38bd55",
   "metadata": {},
   "source": [
    "We can see that the MLP is increasing its performance over time thanks to the backpropagation algorithm. We could obtain better results by increasing the number of epochs, for instance, or with a different network topology.\n",
    "\n",
    "An important aspect that is not supported in this code is that it doesn't allow us to load pre-trained weights in the ``back_propagation()`` method. This is standard procedure when training NNs due to the time it takes to train it. We could for instance, train for 100 epochs, evaluate and store its weights, and then train it again for 100 more epochs to obtain better results.\n",
    "\n",
    "Finally, this extensive code was only shown here for learning purposes, as the ``scikit-learn`` package already gives us an off-the-shelf MLP implementation. Let's check it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3953963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=2000, class_sep = 0.5,\n",
    "                                    n_features=8, n_informative=4, \n",
    "                                    n_redundant=0, n_classes=2)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes = (10, 5), \n",
    "                    activation = 'logistic', solver = 'sgd',\n",
    "                    batch_size = X.shape[0],\n",
    "                    learning_rate_init = 1.0, \n",
    "                    max_iter = 100,\n",
    "                    verbose = False)\n",
    "clf.fit(X,y)\n",
    "pred = clf.predict(X)\n",
    "accuracy = (y == pred).mean()\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57e96a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Neural Networks are connectionist models that were inpired by our neurons\n",
    "- The first widely adopted mathematical model of a neuron was the Rosenblatt's Perceptron, and it is used to this day in many NN topologies. It is a linear model that uses a threshold-based activation.\n",
    "- Multilayer Perceptrons are formed by connecting several layers of perceptrons in a feed-forward architecture. However, they are not strictly Perceptrons because their activation functions are nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7dc66",
   "metadata": {},
   "source": [
    "In our next lesson, we will continue discussing some basic learning techniques to make our models even more efficient. \n",
    "\n",
    "<h1> <center> See you all in our next lesson! &#128516; </center> </h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
